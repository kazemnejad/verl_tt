hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - model@actor_rollout_ref.model: hf_model
  - rollout@actor_rollout_ref.rollout: rollout
  - data@data: legacy_data
  - _self_

trainer:
  n_gpus_per_node: 1
  nnodes: 1
  project_name: entropy_gen_e2e
  experiment_name: test
  default_local_dir: /tmp/entropy_gen_e2e_test
  logger: ["console"]

actor_rollout_ref:
  model:
    path: Qwen/Qwen2.5-0.5B-Instruct
    trust_remote_code: true
  rollout:
    name: sglang
    mode: async
    enforce_eager: true
    tensor_model_parallel_size: 1
    data_parallel_size: 1
    pipeline_model_parallel_size: 1
    gpu_memory_utilization: 0.5
    temperature: 0.0
    top_p: 1.0
    top_k: -1
    prompt_length: 512
    response_length: 256
    n: 1
    skip_tokenizer_init: true
    free_cache_engine: false
    calculate_log_probs: true  # Required for entropy extraction (logprobs=True in sampling_params)
    # Note: entropy_top_k defaults to 0 (full-vocab) via getattr fallback in
    # EntropySGLangHttpServer. We don't set it here because RolloutConfig's
    # structured schema doesn't have this field; the default is correct.
    agent:
      num_workers: 1
      default_agent_loop: entropy_single_turn_agent

reward_model:
  enable: false
  use_reward_loop: false
  enable_resource_pool: false

data:
  train_files: ???  # Set in test -- list of parquet file paths
  train_max_samples: 16
  return_raw_chat: false

train_tasks: null

generation:
  save_batch_size: 10
  pull_timeout: 10.0
  final_merge: true
  show_progress: false
  upload_artifact: false
  custom_manager_cls: treetune_recipe.entropy_gen.agent_loop.EntropyGenerationLoopManager

ray_kwargs:
  ray_init: {}
  timeline_json_file: null
