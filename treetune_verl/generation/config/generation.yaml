defaults:
  - model@actor_rollout_ref.model: hf_model
  - rollout@actor_rollout_ref.rollout: rollout
  - data@data: legacy_data
  - _self_

# === Trainer (hardware + output + logging) ===
trainer:
  n_gpus_per_node: 8
  nnodes: 1
  project_name: generation
  experiment_name: ${now:%Y%m%d_%H%M%S}
  default_local_dir: outputs/${trainer.project_name}/${trainer.experiment_name}
  logger: ["console", "wandb"]

# === Model (inherits from hf_model defaults) ===
actor_rollout_ref:
  model:
    path: ???  # Only override what differs from hf_model defaults
  rollout:
    name: sglang
    tensor_model_parallel_size: 1
    data_parallel_size: 1
    pipeline_model_parallel_size: 1
    gpu_memory_utilization: 0.85
    temperature: 1.0
    top_p: 1.0
    top_k: -1
    prompt_length: 1024
    response_length: 4096
    agent:
      num_workers: 8
      default_agent_loop: single_turn_agent

# === Reward Model (disabled) ===
reward_model:
  enable: false
  use_reward_loop: false
  enable_resource_pool: false

# === Data ===
# Inherits from legacy_data defaults (same as PPO pipeline).
# Uses RLHFDataset via create_rl_dataset() for consistency with training.
# Key fields: train_files, prompt_key, max_prompt_length,
#   return_raw_chat, filter_overlong_prompts, tool_config_path, custom_cls, etc.
# See verl/trainer/config/data/legacy_data.yaml for all available options.
data:
  train_files: ???
  max_prompt_length: 1024
  train_max_samples: -1  # -1 = use full dataset

# === Task System (optional) ===
train_tasks: null

# === Generation Runner ===
generation:
  save_batch_size: 1000
  pull_timeout: 30.0
  final_merge: true
  show_progress: true
  upload_artifact: true

# === Ray (optional) ===
ray_kwargs:
  ray_init: {}
  timeline_json_file: null
